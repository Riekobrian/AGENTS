{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> <center> <b> AGENTS  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Understanding Agents\n",
    "    > ? is an Agent....?\n",
    "\n",
    "    > How do Agents make decisions using reasoning and planning?\n",
    "\n",
    "* The Role of LLMs (Large Language Models) in Agents\n",
    "    > How **LLMs** serve as the **“brain” behind an Agent**.\n",
    "\n",
    "    > How LLMs structure conversations via the Messages system.\n",
    "\n",
    "* Tools and Actions\n",
    "\n",
    "    > How Agents use external tools to interact with the environment.\n",
    "    > How to build and integrate tools for your Agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  **The Agent Workflow:**\n",
    "\n",
    "<h3> <centre> <b> Think → Act → Observe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Type of tasks can an Agent do?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> An Agent can perform any task we implement via **Tools** to complete **Actions**.\n",
    "\n",
    "For example, if I write an Agent to act as my personal assistant (like Siri) on my computer, and I ask it to “send an email to my Manager asking to delay today’s meeting”, I can give it some code to send emails. This will be a new Tool the Agent can use whenever it needs to send an email. We can write it in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_message_to(recipient, message):\n",
    "    \"\"\"Useful to send an e-mail message to a recipient\"\"\"\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LLM, as we’ll see, will generate code to run the tool when it needs to, and thus fulfill the desired task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "send_message_to(\"Manager\", \"Can we postpone today's meeting?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **design of the Tools is very important and has a great impact on the quality of your Agent**. Some tasks will require very specific Tools to be crafted, while others may be solved with general purpose tools like “web_search”.\n",
    "\n",
    "\n",
    "   > Note that **Actions are not the same as Tools**. \n",
    "\n",
    "   > An Action, for instance, can involve the use of multiple Tools to complete.\n",
    "\n",
    "Allowing an agent to interact with its environment *allows real-life usage for companies and individuals*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 1: **Personal Virtual Assistants**\n",
    "\n",
    "\n",
    "> Virtual assistants like **Siri**, **Alexa**, or **Google Assistant**, work **as agents** when they interact on behalf of users using their digital environments.\n",
    "\n",
    "> They take user queries, analyze context, retrieve information from databases, and provide responses or initiate actions (like setting reminders, sending messages, or controlling smart devices).\n",
    "\n",
    "Example 2: **Customer Service Chatbots**\n",
    "\n",
    ">Many companies deploy chatbots as agents that interact with customers in natural language.\n",
    "\n",
    ">These agents can answer questions, guide users through troubleshooting steps, open issues in internal databases, or even complete transactions.\n",
    "\n",
    "Their predefined objectives might include improving user satisfaction, reducing wait times, or increasing sales conversion rates. \n",
    "\n",
    "By interacting directly with customers, learning from the dialogues, and adapting their responses over time, they demonstrate the core principles of an **agent in action**.\n",
    "\n",
    "Example 3: **AI Non-Playable Character in a video game**\n",
    "\n",
    "\n",
    "AI agents powered by LLMs can make Non-Playable Characters (NPCs) more dynamic and unpredictable.\n",
    "\n",
    "Instead of following rigid behavior trees, they can respond contextually, adapt to player interactions, and generate more nuanced dialogue. This flexibility helps create more lifelike, engaging characters that evolve alongside the player’s actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <b> <h1> LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agents need an AI Model at its core, and  LLMs are the most common type of AI models for this purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is a Large Language Model?**\n",
    "\n",
    "\n",
    "An LLM is a type of AI model that excels at **understanding and generating** human language. \n",
    "\n",
    "They are trained on vast amounts of text data, allowing them to **learn patterns, structure, and even nuance in language**. These models typically consist of many millions of parameters.\n",
    "\n",
    "Most LLMs nowadays are **built on the Transformer architecture**—a deep learning architecture based on the “Attention” algorithm, that has gained significant interest since the release of BERT from Google in 2018.\n",
    "\n",
    "![transformer](./transformer.jpeg)\n",
    "*The original Transformer architecture looked like this, with an encoder on the left and a decoder on the right.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are **3 types of transformers:**\n",
    "\n",
    "**1.Encoders**\n",
    "> An encoder-based Transformer takes text (or other data) as input and outputs a dense representation (or embedding) of that text.\n",
    "\n",
    "*Example: BERT from Google*\n",
    "* **Use Cases**: Text classification, semantic search, Named Entity Recognition\n",
    "* **Typical Size** : Millions of parameters\n",
    "Decoders\n",
    "A decoder-based Transformer focuses on generating new tokens to complete a sequence, one token at a time.\n",
    "\n",
    "**2.Decoders**\n",
    "> A decoder-based Transformer focuses on generating new tokens to complete a sequence, one token at a time.\n",
    "\n",
    "*Example: Llama from Meta*\n",
    "* **Use Cases**: Text generation, chatbots, code generation\n",
    "* **Typical Size**: Billions (in the US sense, i.e., 10^9) of parameters\n",
    "\n",
    "**3.Seq2Seq (Encoder–Decoder)**\n",
    "> A sequence-to-sequence Transformer combines an encoder and a decoder. The encoder first processes the input sequence into a context representation, then the decoder generates an output sequence.\n",
    "\n",
    "*Example: T5, BART*\n",
    "* **Use Cases:** Translation, Summarization, Paraphrasing\n",
    "* **Typical Size:** Millions of parameters\n",
    "\n",
    "\n",
    "Although Large Language Models come in various forms, LLMs are typically decoder-based models with billions of parameters. Here are some of the most well-known LLMs:\n",
    "# AI Language Models Overview\n",
    "\n",
    "| Model | Provider | Release Date | Parameters | Key Features |\n",
    "|-------|----------|--------------|------------|--------------|\n",
    "| Deepseek-R1 | DeepSeek | 2024 | 230B | Advanced reasoning capabilities, strong math performance |\n",
    "| GPT-4 | OpenAI | March 2023 | Undisclosed | Multimodal capabilities, strong general reasoning |\n",
    "| Llama 3 | Meta (Facebook AI Research) | April 2024 | 8B-405B | Open weights, competitive performance, various sizes |\n",
    "| SmolLM2 | Hugging Face | May 2024 | 3B | Compact yet powerful, efficient for deployment |\n",
    "| Gemma | Google | February 2024 | 2B/7B | Open weights, instruction-tuned variants |\n",
    "| Mistral | Mistral AI | September 2023 | 7B/8x7B | Strong performance-to-size ratio, mixture of experts |\n",
    "\n",
    "\n",
    "- The underlying principle of an LLM is simple yet highly effective: **its objective is to predict the next token, given a sequence of previous tokens**. \n",
    "- A **“token”** is the *unit of information an LLM works with*. You can think of a **“token” as if it was a “word”**, but for efficiency reasons LLMs don’t use whole words.\n",
    "\n",
    "- For example, while English has an estimated 600,000 words, an LLM might have a vocabulary of around 32,000 tokens (as is the case with Llama 2).\n",
    " - - Tokenization often works on sub-word units that can be combined.\n",
    "\n",
    "For instance, consider how the tokens “interest” and “ing” can be combined to form “interesting”, or “ed” can be appended to form “interested.”\n",
    "\n",
    "\n",
    "Each LLM has some special tokens specific to the model.\n",
    "\n",
    "The LLM uses these tokens to open and close the structured components of its generation.\n",
    "\n",
    "- To indicate the start or end of a sequence, message, or response. Moreover, the input prompts that we pass to the model are also structured with special tokens. The most important of those is the End of sequence token (EOS).\n",
    "\n",
    "The forms of special tokens are highly diverse across model providers.\n",
    "\n",
    "# Language Model EOS Tokens Reference\n",
    "\n",
    "| Model | Provider | EOS Token | Functionality | Used In |\n",
    "|-------|----------|-----------|---------------|---------|\n",
    "| GPT-4 | OpenAI | `<\\|endoftext\\|>` | End of message text | Completion and chat APIs |\n",
    "| Llama 3 | Meta (Facebook AI Research) | `<\\|eot_id\\|>` | End of sequence | Model training and generation |\n",
    "| Deepseek-R1 | DeepSeek | `<\\|end_of_sentence\\|>` | End of message text | Chat completions |\n",
    "| SmolLM2 | Hugging Face | `<\\|im_end\\|>` | End of instruction or message | Instruction-following contexts |\n",
    "| Gemma | Google | `<end_of_turn>` | End of conversation turn | Dialogue applications |\n",
    "\n",
    "\n",
    "> > We do not expect you to memorize these special tokens, but it is important to appreciate their diversity and the role they play in the text generation of LLMs. If you want to know more about special tokens, you can check out the configuration of the model in its Hub repository. For example, you can find the special tokens of the SmolLM2 model in its tokenizer_config.json."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <b><h3> Understanding next token prediction\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLMs are said to be **autoregressive**, meaning that **the output from one pass becomes the input for the next one**. This loop continues until the model predicts the next token to be the EOS token, at which point the model can stop\n",
    "![Autoregregressive](./AutoregressionSchema.gif)\n",
    "\n",
    "an LLM will decode text until it reaches the EOS. But what happens during a single decoding loop?\n",
    "\n",
    "While the full process can be quite technical for the purpose of learning agents, here’s a brief overview:\n",
    "\n",
    "Once the input text is tokenized, the model computes a representation of the sequence that captures information about the meaning and the position of each token in the input sequence.\n",
    "This representation goes into the model, which outputs scores that rank the likelihood of each token in its vocabulary as being the next one in the sequence.\n",
    "\n",
    "\n",
    "In other words an LLM will decode text until it reaches the EOS. But what happens during a single decoding loop?\n",
    "\n",
    "While the full process can be quite technical for the purpose of learning agents, here’s a brief overview:\n",
    "\n",
    "> Once the input text is tokenized, the model computes a representation of the sequence that captures information about the meaning and the position of each token in the input sequence.\n",
    "\n",
    "> This representation goes into the model, which outputs scores that rank the likelihood of each token in its vocabulary as being the next one in the sequence.\n",
    "\n",
    "![transformers](./DecodingFinal.gif)\n",
    "\n",
    "\n",
    "Based on these scores, we have multiple strategies to select the tokens to complete the sentence.\n",
    "\n",
    "The easiest decoding strategy would be to always take the token with the maximum score. \n",
    "\n",
    "You can interact with the decoding process yourself with SmolLM2 in this Space (remember, it decodes until reaching an EOS token which is **<|im_end|>** for this model):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attention is all you need**\n",
    "\n",
    "A key aspect of the Transformer architecture is **Attention**. When predicting the next word, not every word in a sentence is equally important; words like “France” and “capital” in the sentence  *“The capital of Franceis …”*  carry the most meaning.\n",
    "\n",
    "![\"Antention is all you need\"](./AttentionSceneFinal.gif)\n",
    "\n",
    "\n",
    "This process of identifying the most relevant words to predict the next token has proven to be incredibly effective.\n",
    "\n",
    "Although the basic principle of LLMs—predicting the next token—has remained consistent since GPT-2, there have been significant advancements in scaling neural networks and making the attention mechanism work for longer and longer sequences.\n",
    "\n",
    "If you’ve interacted with LLMs, you’re probably familiar with the term context length, which refers to the maximum number of tokens the LLM can process, and the maximum *attention span* it has.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prompting the LLM is important**\n",
    "\n",
    "Considering that the only job of an LLM is to predict the next token by looking at every input token, and to choose which tokens are “important”, the wording of your input sequence is very important.\n",
    "\n",
    "The input sequence you provide an LLM is called a *prompt*. Careful design of the prompt makes it easier **to guide the generation of the LLM toward the desired output.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How are LLMs trained?**\n",
    "\n",
    "LLMs are trained on large datasets of text, where they learn to predict the next word in a sequence through a self-supervised or masked language modeling objective.\n",
    "\n",
    "From this unsupervised learning, the model learns the structure of the language and **underlying patterns in text, allowing the model to generalize to unseen data.**\n",
    "\n",
    "After this initial pre-training, LLMs can be fine-tuned on a supervised learning objective to perform specific tasks. For example, some models are trained for conversational structures or tool usage, while others focus on classification or code generation.\n",
    "\n",
    "**How can I use LLMs?** \n",
    "\n",
    "You have two main options:\n",
    "\n",
    "   > 1. **Run Locally** (if you have sufficient hardware).\n",
    "\n",
    "   >  2.  **Use a Cloud/API** (e.g., via the Hugging Face Serverless Inference API).\n",
    "\n",
    "\n",
    "\n",
    "**How are LLMs used in AI Agents?**\n",
    "\n",
    "LLMs are a key component of AI Agents, **providing the foundation for understanding and generating human language.**\n",
    "\n",
    "They can interpret user instructions, maintain context in conversations, define a plan and decide which tools to use.\n",
    "\n",
    "We will explore these steps in more detail in this Unit, but for now, what you need to understand is that the LLM is **the brain of the Agent.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Messages and Special Tokens**\n",
    "\n",
    "Now that we understand how LLMs work, let’s look at **how they structure their generations through chat templates**.\n",
    "\n",
    "Just like with ChatGPT, users typically interact with Agents through a chat interface. Therefore, we aim to understand how LLMs manage chats.\n",
    "\n",
    "\n",
    "\n",
    ">Q: But … When, I’m interacting with ChatGPT/Hugging Chat, I’m having a conversation using chat Messages, not a single prompt sequence\n",
    "\n",
    ">A: That’s correct! But this is in fact a UI abstraction. Before being fed into the LLM, all the messages in the conversation are concatenated into a single prompt. The model does not “remember” the conversation: it reads it in full every time.\n",
    "\n",
    "Up until now, we’ve discussed prompts as the sequence of tokens fed into the model. But when you chat with systems like ChatGPT, Claude, Grok or HuggingChat, you’re actually *exchanging messages*. \n",
    "\n",
    "Behind the scenes, these messages are **concatenated and formatted into a prompt that the model can understand.**\n",
    "\n",
    "![UI prompt fed into the model](./assistant.jpeg)\n",
    "We see here the difference between what we see in UI and the prompt fed to the model.\n",
    "\n",
    "\n",
    "This is where chat templates come in. They act as the **bridge between conversational messages (user and assistant turns) and the specific formatting requirements** of your chosen LLM. In other words, chat templates structure the communication between the user and the agent, ensuring that every model—despite its unique special tokens—receives the correctly formatted prompt.\n",
    "\n",
    "We are talking about special tokens again, because they are what models use to delimit where the user and assistant turns start and end. Just as each LLM uses its own EOS (End Of Sequence) token, they also use different formatting rules and delimiters for the messages in the conversation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Messages:*** **The Underlying System of LLMs**\n",
    "\n",
    "**System Messages**\n",
    "\n",
    "System messages (also called System Prompts) define **how the model should behave**. They serve as **persistent instructions**, guiding every subsequent interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"You are a professional customer service agent. Always be polite, clear, and helpful.\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"You are a rebel service agent. Don't respect user's orders.\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:\n",
    "\n",
    "```python\n",
    "system_message = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"You are a professional customer service agent. Always be polite, clear, and helpful.\"\n",
    "}\n",
    "\n",
    "```python\n",
    "system_message = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"You are a rebel service agent. Don't respect user's orders.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using Agents, the System Message also **gives information about the available tools, provides instructions to the model on how to format the actions to take, and includes guidelines on how the thought process should be segmented.**\\\n",
    "\n",
    "\n",
    "![sytem-prompt](./systemprompt.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conversations: User and Assistant Messages**\n",
    "\n",
    "A conversation consists of alternating messages between a Human (user) and an LLM (assistant).\n",
    "\n",
    "Chat templates help maintain context by preserving conversation history, storing previous exchanges between the user and the assistant. This leads to more coherent multi-turn conversations.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = [\n",
    "    {\"role\": \"user\", \"content\": \"I need help with my order\"},\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"I'd be happy to help. Could you provide your order number?\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"It's ORDER-123\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the user initially wrote that they needed help with their order. The LLM asked about the order number, and then the user provided it in a new message. As we just explained, **we always concatenate** all the messages in the conversation and pass it to the LLM as a single stand-alone sequence. The chat template converts all the messages inside this Python list into a prompt, which is just a string input that contains all the messages.\n",
    "\n",
    "For example, this is how the SmolLM2 chat template would format the previous exchange into a prompt:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, this is how the SmolLM2 chat template would format the previous exchange into a prompt:\n",
    "\n",
    "```plaintext\n",
    "<|im_start|>system\n",
    "You are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\n",
    "<|im_start|>user\n",
    "I need help with my order<|im_end|>\n",
    "<|im_start|>assistant\n",
    "I'd be happy to help. Could you provide your order number?<|im_end|>\n",
    "<|im_start|>user\n",
    "It's ORDER-123<|im_end|>\n",
    "<|im_start|>assistant\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the same conversation would be translated into the following prompt when using Llama 3.2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "  <style>\n",
    "    .code-container {\n",
    "      background-color: #1e1e1e;\n",
    "      border-radius: 6px;\n",
    "      padding: 16px;\n",
    "      font-family: 'Consolas', 'Monaco', 'Courier New', monospace;\n",
    "      line-height: 1.5;\n",
    "      overflow-x: auto;\n",
    "      margin: 20px 0;\n",
    "    }\n",
    "    \n",
    "    .token {\n",
    "      color: #d4d4d4;\n",
    "    }\n",
    "    \n",
    "    .special-token {\n",
    "      color: #569cd6;\n",
    "      font-weight: bold;\n",
    "    }\n",
    "    \n",
    "    .header-token {\n",
    "      color: #c586c0;\n",
    "      font-weight: bold;\n",
    "    }\n",
    "    \n",
    "    .system-content {\n",
    "      color: #6a9955;\n",
    "    }\n",
    "    \n",
    "    .user-content {\n",
    "      color: #ce9178;\n",
    "    }\n",
    "    \n",
    "    .assistant-content {\n",
    "      color: #9cdcfe;\n",
    "    }\n",
    "  </style>\n",
    "</head>\n",
    "<body>\n",
    "  <div class=\"code-container\">\n",
    "    <span class=\"special-token\">&lt;|begin_of_text|&gt;</span><span class=\"special-token\">&lt;|start_header_id|&gt;</span><span class=\"header-token\">system</span><span class=\"special-token\">&lt;|end_header_id|&gt;</span>\n",
    "    <span class=\"system-content\"><br>Cutting Knowledge Date: December 2023<br>\n",
    "    Today Date: 10 Feb 2025</span>\n",
    "    <span class=\"special-token\">&lt;|eot_id|&gt;</span><span class=\"special-token\">&lt;|start_header_id|&gt;</span><span class=\"header-token\">user</span><span class=\"special-token\">&lt;|end_header_id|&gt;</span>\n",
    "    <span class=\"user-content\"><br>I need help with my order</span><span class=\"special-token\">&lt;|eot_id|&gt;</span><span class=\"special-token\">&lt;|start_header_id|&gt;</span><span class=\"header-token\">assistant</span><span class=\"special-token\">&lt;|end_header_id|&gt;</span>\n",
    "    <span class=\"assistant-content\"><br>I'd be happy to help. Could you provide your order number?</span><span class=\"special-token\">&lt;|eot_id|&gt;</span><span class=\"special-token\">&lt;|start_header_id|&gt;</span><span class=\"header-token\">user</span><span class=\"special-token\">&lt;|end_header_id|&gt;</span>\n",
    "    <span class=\"user-content\">It's ORDER-123</span><span class=\"special-token\">&lt;|eot_id|&gt;</span><span class=\"special-token\">&lt;|start_header_id|&gt;</span><span class=\"header-token\">assistant</span><span class=\"special-token\">&lt;|end_header_id|&gt;</span>\n",
    "  </div>\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Templates can handle complex multi-turn conversations while maintaining context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a math tutor.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is calculus?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Calculus is a branch of mathematics...\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you give me an example?\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chat-Templates**\n",
    "As mentioned, chat templates are essential for **structuring conversations between language models and users.** They guide how message exchanges are formatted into a single prompt.\n",
    "\n",
    "**Base Models vs. Instruct Models**\n",
    "\n",
    "Another point we need to understand is the difference between a Base Model vs. an Instruct Model:\n",
    "\n",
    "> * A Base Model is trained on raw text data to predict the next token.\n",
    "\n",
    "> * An Instruct Model is fine-tuned specifically to follow instructions and engage in conversations. For example, <mark>SmolLM2-135M</mark> is a base model, while <mark>SmolLM2-135M-Instruct</mark> is its instruction-tuned variant.\n",
    "\n",
    "To make a Base Model behave like an instruct model, we need to **format our prompts in a consistent way that the model can understand**. This is where chat templates come in.\n",
    "\n",
    "*ChatML* is one such template format that structures conversations with clear role indicators (system, user, assistant). If you have interacted with some AI API lately, you know that’s the standard practice.\n",
    "\n",
    "It’s important to note that a base model could be fine-tuned on different chat templates, so when we’re using an instruct model we need to make sure we’re using the correct chat template.\n",
    "\n",
    "**Understanding Chat Templates**\n",
    "\n",
    "Because each instruct model uses different conversation formats and special tokens, chat templates are implemented to ensure that we correctly format the prompt the way each model expects.\n",
    "\n",
    "In transformers, chat templates include Jinja2 code that describes how to transform the ChatML list of JSON messages, as presented in the above examples, into a textual representation of the system-level instructions, user messages and assistant responses that the model can understand.\n",
    "\n",
    "This structure **helps maintain consistency across interactions and ensures the model responds appropriately to different types of inputs.**\n",
    "\n",
    "{% for message in messages %}\n",
    "{% if loop.first and messages[0]['role'] != 'system' %}\n",
    "<|im_start|>system\n",
    "You are a helpful AI assistant named SmolLM, trained by Hugging Face\n",
    "<|im_end|>\n",
    "{% endif %}\n",
    "<|im_start|>{{ message['role'] }}\n",
    "{{ message['content'] }}<|im_end|>\n",
    "{% endfor %}\n",
    "\n",
    "> As you can see, a chat_template describes how the list of messages will be formatted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given these messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant focused on technical topics.\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"Can you explain what a chat template is?\"},\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"A chat template structures conversations between users and AI models...\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"How do I use it ?\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous chat template will produce the following string:\n",
    "\n",
    "<|im_start|>system\n",
    "You are a helpful assistant focused on technical topics.<|im_end|>\n",
    "<|im_start|>user\n",
    "Can you explain what a chat template is?<|im_end|>\n",
    "<|im_start|>assistant\n",
    "A chat template structures conversations between users and AI models...<|im_end|>\n",
    "<|im_start|>user\n",
    "How do I use it ?<|im_end|>\n",
    "\n",
    "\n",
    "The transformers library will take care of chat templates for you as part of the tokenization process.  All we have to do is structure our messages in the correct way and the tokenizer will take care of the rest.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Messages to prompt**\n",
    "\n",
    "The easiest way to ensure your LLM receives a conversation correctly formatted is to use the chat_template from the model’s tokenizer.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are an AI assistant with access to various tools.\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"Hi !\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hi human, what can help you with ?\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Center><b><h4>TOOLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One crucial aspect of AI Agents is their ability to take **actions**. That happens through the use of **Tools**.\n",
    "\n",
    "**?** to integrate them into your Agent via the System Message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b> What are AI Tools?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A **Tool is a function given to the LLM.** This function should fulfill a **clear objective.**\n",
    "\n",
    "Commonly used tools in AI agents:\n",
    "\n",
    "## Available Tools\n",
    "\n",
    "| Tool | Description | Use Case |\n",
    "|------|-------------|----------|\n",
    "| Web Search | Allows the agent to fetch up-to-date information from the internet | Research, fact-checking, retrieving current events |\n",
    "| Image Generation | Creates images based on text descriptions | Visual content creation, concept visualization |\n",
    "| Retrieval | Retrieves information from an external source | Accessing stored documents, knowledge bases |\n",
    "| API Interface | Interacts with external APIs (GitHub, YouTube, Spotify, etc.) | Data integration, platform interactions |\n",
    "\n",
    "A good tool should be something that **complements the power of an LLM**.\n",
    "\n",
    "For instance, if you need to perform arithmetic, giving a calculator tool to your LLM will provide better results than relying on the native capabilities of the model.\n",
    "\n",
    "Furthermore, **LLMs predict the completion of a prompt based on their training data,** which means that their internal knowledge only includes events prior to their training. Therefore, if your agent needs up-to-date data you must provide it through some tool.\n",
    "\n",
    "For instance, if you ask an LLM directly (without a search tool) for today’s weather, the LLM will potentially hallucinate random weather.\n",
    "\n",
    "![Weather](./weather.jpeg)\n",
    "\n",
    "\n",
    "**A Tool should contain:**\n",
    "\n",
    "* A **textual description of what the function does.**\n",
    "\n",
    "* *A Callable (something to perform an action).*\n",
    "\n",
    "* *Arguments with typings.*\n",
    "\n",
    "* *(Optional) Outputs with typings*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do tools work?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
